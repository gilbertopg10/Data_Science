{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()    \n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "api_key = os.getenv('yt_api_key')\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key\n",
    ")\n",
    "\n",
    "request = youtube.channels().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=\"UCX6OQ3DkcsbYNE6H8uQQuVA\"\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "\n",
    "# print the response in json format \n",
    "JSON(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Summary stats of the channel\n",
    "\n",
    "# Extract the relevant data\n",
    "channel_data = response['items'][0]\n",
    "snippet = channel_data['snippet']\n",
    "statistics = channel_data['statistics']\n",
    "content_details = channel_data['contentDetails']['relatedPlaylists']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Channel ID\": [channel_data['id']],\n",
    "    \"Title\": [snippet['title']],\n",
    "    \"Description\": [snippet['description']],\n",
    "    \"Custom URL\": [snippet['customUrl']],\n",
    "    \"Published At\": [snippet['publishedAt']],\n",
    "    \"Country\": [snippet.get('country', 'N/A')],\n",
    "    \"Subscriber Count\": [statistics['subscriberCount']],\n",
    "    \"View Count\": [statistics['viewCount']],\n",
    "    \"Video Count\": [statistics['videoCount']],\n",
    "    \"Uploads Playlist ID\": [content_details['uploads']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId = playlist_id,\n",
    "                maxResults = 50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                        part='contentDetails',\n",
    "                        playlistId = playlist_id,\n",
    "                        maxResults = 50,\n",
    "                        pageToken = next_page_token)\n",
    "            response = request.execute()\n",
    "    \n",
    "            for i in range(len(response['items'])):\n",
    "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809\n"
     ]
    }
   ],
   "source": [
    "# get the list of videos\n",
    "\n",
    "videos_list = get_video_ids(youtube, playlist_id='UUX6OQ3DkcsbYNE6H8uQQuVA')\n",
    "\n",
    "print(len(videos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "        \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "            \n",
    "    return pd.DataFrame(all_video_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_video_details(youtube, videos_list)\n",
    "\n",
    "# print dataframe shape\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA dataframe\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head dataframe\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "# Step 1: Convert columns to numeric types and create df_2\n",
    "df_2 = df.copy()\n",
    "df_2['viewCount'] = pd.to_numeric(df_2['viewCount'], errors='coerce')\n",
    "df_2['likeCount'] = pd.to_numeric(df_2['likeCount'], errors='coerce')\n",
    "df_2['commentCount'] = pd.to_numeric(df_2['commentCount'], errors='coerce')\n",
    "\n",
    "# Step 2: Drop the 'favouriteCount' column and save to df_2\n",
    "df_2.drop(columns=['favouriteCount'], inplace=True)\n",
    "\n",
    "# Step 3: Convert 'publishedAt' to datetime format and save to df_2\n",
    "df_2['publishedAt'] = pd.to_datetime(df_2['publishedAt'])\n",
    "\n",
    "# Step 4: Extract the day of the week from 'publishedAt' and save to df_2\n",
    "df_2['day_of_week'] = df_2['publishedAt'].dt.day_name()\n",
    "\n",
    "# Step 5: Convert 'duration' to total seconds and save to df_2\n",
    "df_2['duration'] = df_2['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
    "\n",
    "# Step 6: Fill NaN values for specific columns and save to df_2\n",
    "df_2['tags'].fillna('', inplace=True)\n",
    "df_2['description'].fillna('', inplace=True)\n",
    "\n",
    "# Display the first few rows of df_2 to verify changes\n",
    "print(df_2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the 'dataset' folder\n",
    "folder_path = 'dataset'\n",
    "\n",
    "# Create the 'dataset' folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "file_path = os.path.join(folder_path, 'mrbeast_channel.csv')\n",
    "\n",
    "# Save df_2 to a CSV file\n",
    "df_2.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
